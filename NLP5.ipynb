{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What are Sequence-to-sequence models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sequence to Sequence (often abbreviated to seq2seq) models is a special class of Recurrent Neural Network architectures that we typically use (but not restricted) to solve complex Language problems like Machine Translation, Question Answering, creating Chatbots, Text Summarization, etc.\n",
    "\n",
    "The most common architecture used to build Seq2Seq models is Encoder-Decoder architecture.\n",
    "\n",
    "The LSTM encoder and decoder are used to process the sequence to sequence modelling in this task. two ways to use LSTM networks for regression: sequence-to-sequence: The output of the LSTM layer is a sequence, fed into a fully connected layer. lstmLayer(N, 'OutputMode', 'sequence' ). sequence-to-one: The output of the LSTM layer is the last element of the sequence, fed into a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b51f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What are the Problem with Vanilla RNNs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vanilla RNNs suffer from the problem of vanishing gradients, which hampers learning of long data sequences. The gradients carry information used in the RNN parameter update and when the gradient becomes smaller and smaller, the parameter updates become insignificant which means no real learning is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2add3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. What is Gradient clipping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd04b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient clipping ensures the gradient vector g has norm at most c. This helps gradient descent to have a reasonable behaviour even if the loss landscape of the model is irregular.\n",
    "\n",
    "Gradient clipping is a technique to prevent exploding gradients in very deep networks, usually in recurrent neural networks. A neural network is a learning algorithm, also called neural network or neural net, that uses a network of functions to understand and translate data input into a specific output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53de0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Explain Attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5c2b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is Attention?\n",
    "In psychology, attention is the cognitive process of selectively concentrating on one or a few things while ignoring others.\n",
    "\n",
    "A neural network is considered to be an effort to mimic human brain actions in a simplified manner. Attention Mechanism is also an attempt to implement the same action of selectively concentrating on a few relevant things, while ignoring others in deep neural networks.\n",
    "\n",
    "Let me explain what this means. Let’s say you are seeing a group photo of your first school. Typically, there will be a group of children sitting across several rows, and the teacher will sit somewhere in between. Now, if anyone asks the question, “How many people are there?”, how will you answer it?\n",
    "\n",
    "Simply by counting heads, right? You don’t need to consider any other things in the photo. Now, if anyone asks a different question, “Who is the teacher in the photo?”, your brain knows exactly what to do. It will simply start looking for the features of an adult in the photo. The rest of the features will simply be ignored. This is the ‘Attention’ which our brain is very adept at implementing.\n",
    "\n",
    "How Attention Mechanism was Introduced in Deep Learning\n",
    "The attention mechanism emerged as an improvement over the encoder decoder-based neural machine translation system in natural language processing (NLP). Later, this mechanism, or its variants, was used in other applications, including computer vision, speech processing, etc.\n",
    "\n",
    "Before Bahdanau et al proposed the first Attention model in 2015, neural machine translation was based on encoder-decoder RNNs/LSTMs. Both encoder and decoder are stacks of LSTM/RNN units. It works in the two following steps:\n",
    "\n",
    "The encoder LSTM is used to process the entire input sentence and encode it into a context vector, which is the last hidden state of the LSTM/RNN. This is expected to be a good summary of the input sentence. All the intermediate states of the encoder are ignored, and the final state id supposed to be the initial hidden state of the decoder\n",
    "\n",
    "The decoder LSTM or RNN units produce the words in a sentence one after another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b95a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Explain Conditional random fields (CRFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f7de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \"neighboring\" samples, a CRF can take context into account. To do so, the prediction is modeled as a graphical model, which implements dependencies between the predictions. What kind of graph is used depends on the application. For example, in natural language processing, linear chain CRFs are popular, which implement sequential dependencies in the predictions. In image processing the graph typically connects locations to nearby and/or similar locations to enforce that they receive similar predictions.\n",
    "\n",
    "Other examples where CRFs are used are: labeling or parsing of sequential data for natural language processing or biological sequences, POS tagging, shallow parsing, named entity recognition, gene finding, peptide critical functional region finding, and object recognition and image segmentation in computer vision.\n",
    "\n",
    "Conditional Random Fields is a class of discriminative models best suited to prediction tasks where contextual information or state of the neighbors affect the current prediction. CRFs find their applications in named entity recognition, part of speech tagging, gene prediction, noise reduction and object detection problems, to name a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f28caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Explain self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cd8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Self-attention mechanism:\n",
    "\n",
    "\n",
    "\n",
    "The attention mechanism allows output to focus attention on input while producing output while the self-attention model allows inputs to interact with each other (i.e calculate attention of all other inputs wrt one input.\n",
    "\n",
    "The first step is multiplying each of the encoder input vectors with three weights matrices (W(Q), W(K), W(V)) that we trained during the training process. This matrix multiplication will give us three vectors for each of the input vector: the key vector, the query vector, and the value vector.\n",
    "The second step in calculating self-attention is to multiply the Query vector of the current input with the key vectors from other inputs.\n",
    "In the third step, we will divide the score by square root of dimensions of the key vector (dk). In the paper the dimension of the key vector is 64, so that will be 8. The reason behind that is if the dot products become large, this causes some self-attention scores to be very small after we apply softmax function in the future.\n",
    "In the fourth step, we will apply the softmax function on all self-attention scores we calculated wrt the query word (here first word).\n",
    "In the fifth step, we multiply the value vector on the vector we calculated in the previous step.\n",
    "In the final step, we sum up the weighted value vectors that we got in the previous step, this will give us the self-attention output for the given word.\n",
    "The above procedure is applied to all the input sequences. Mathematically, the self-attention matrix for input matrices (Q, K, V) is calculated as:\n",
    "\n",
    " where Q, K, V are the concatenation of query, key, and value vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825bd15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What is Bahdanau Attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f02c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed an attention mechanism that learns to align and translate jointly. It is also known as Additive attention as it performs a linear combination of encoder states and the decoder states.\n",
    "\n",
    "image.png let’s understand the Attention mechanism suggested by Bahdanau\n",
    "\n",
    "All hidden states of the encoder(forward and backward) and the decoder are used to generate the context vector, unlike how just the last encoder hidden state is used in seq2seq without attention.\n",
    "The attention mechanism aligns the input and output sequences, with an alignment score parameterized by a feed-forward network. It helps to pay attention to the most relevant information in the source sequence.\n",
    "The model predicts a target word based on the context vectors associated with the source position and the previously generated target words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. What is a Language Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb64ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are primarily two types of language models:\n",
    "\n",
    "Statistical Language Models.\n",
    "Neural Language Models.\n",
    "Speech Recognization.\n",
    "Machine Translation.\n",
    "Sentiment Analysis.\n",
    "Text Suggestions.\n",
    "Parsing Tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bafbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. What is Multi-Head Attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76458e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multi-head Attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. What is Bilingual Evaluation Understudy (BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ea9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU (BiLingual Evaluation Understudy) is a metric for automatically evaluating machine-translated text. The BLEU score is a number between zero and one that measures the similarity of the machine-translated text to a set of high quality reference translations. A value of 0 means that the machine-translated output has no overlap with the reference translation (low quality) while a value of 1 means there is perfect overlap with the reference translations (high quality).\n",
    "\n",
    "It has been shown that BLEU scores correlate well with human judgment of translation quality. Note that even human translators do not achieve a perfect score of 1.0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

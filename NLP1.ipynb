{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Explain One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71dc5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Categorical data refers to variables that are made up of label values, for example, a “color” variable could have the values “red“, “blue, and “green”. Think of values like different categories that sometimes have a natural ordering to them.\n",
    "\n",
    "Some machine learning algorithms can work directly with categorical data depending on implementation, such as a decision tree, but most require any inputs or outputs variables to be a number, or numeric in value. This means that any categorical data must be mapped to integers.\n",
    "\n",
    "One hot encoding is one method of converting data to prepare it for an algorithm and get a better prediction. With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector. All the values are zero, and the index is marked with a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Explain Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bff3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms.\n",
    "\n",
    "The approach is very simple and flexible, and can be used in a myriad of ways for extracting features from documents.\n",
    "\n",
    "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "A vocabulary of known words.\n",
    "A measure of the presence of known words.\n",
    "It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.\n",
    "\n",
    "A very common feature extraction procedures for sentences and documents is the bag-of-words approach (BOW). In this approach, we look at the histogram of the words within the text, i.e. considering each word count as a feature.\n",
    "\n",
    "The intuition is that documents are similar if they have similar content. Further, that from the content alone we can learn something about the meaning of the document.\n",
    "\n",
    "The bag-of-words can be as simple or complex as you like. The complexity comes both in deciding how to design the vocabulary of known words (or tokens) and how to score the presence of known words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e69b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Explain Bag of N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a98dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A bag-of- n -grams model is a way to represent a document, similar to a [bag-of-words][/terms/bag-of-words/] model.\n",
    "\n",
    "A bag-of- n -grams model represents a text document as an unordered collection of its n -grams.\n",
    "\n",
    "For example, let’s use the following phrase and divide it into bi-grams (\n",
    "\n",
    "n\n",
    "2 ).\n",
    "\n",
    "James is the best person ever.\n",
    "\n",
    "becomes\n",
    "\n",
    "James\n",
    "James is\n",
    "is the\n",
    "the best\n",
    "best person\n",
    "person ever.\n",
    "ever.\n",
    "In a typical bag-of-n-grams model, these 6 bigrams would be a sample from a large number of bigrams observed in a corpus. And then James is the best person ever. would be encoded in a representation showing which of the corpus’s bigrams were observed in the sentence.\n",
    "\n",
    "A bag-of-n-grams model has the simplicity of the bag-of-words model, but allows the preservation of more word locality information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e68595",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Explain TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2db64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.\n",
    "\n",
    "This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n",
    "\n",
    "It has many uses, most importantly in automated text analysis, and is very useful for scoring words in machine learning algorithms for Natural Language Processing (NLP).\n",
    "\n",
    "TF-IDF was invented for document search and information retrieval. It works by increasing proportionally to the number of times a word appears in a document, but is offset by the number of documents that contain the word. So, words that are common in every document, such as this, what, and if, rank low even though they may appear many times, since they don’t mean much to that document in particular.\n",
    "\n",
    "However, if the word Bug appears many times in a document, while not appearing many times in others, it probably means that it’s very relevant. For example, if what we’re doing is trying to find out which topics some NPS responses belong to, the word Bug would probably end up being tied to the topic Reliability, since most responses containing that word would be about that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What is OOV problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1906ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A weakness in any machine translation system is its ability to translate unknown or Out-Of-Vocabulary (OOV) words, those that do not occur in the training data. Nematus is an upcoming NMT system that has a built-in model to deal with the OOV word problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67ea210",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What are word embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c350f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. The term word2vec literally translates to word to vector. For example,\n",
    "\n",
    "“dad” = [0.1548, 0.4848, …, 1.864]\n",
    "\n",
    "“mom” = [0.8785, 0.8974, …, 2.794]\n",
    "\n",
    "The most important feature of word embeddings is that similar words in a semantic sense have a smaller distance (either Euclidean, cosine or other) between them than words that have no semantic relationship. For example, words like “mom” and “dad” should be closer together than the words “mom” and “ketchup” or “dad” and “butter”.\n",
    "\n",
    "The most important feature of word embeddings is that similar words in a semantic sense have a smaller distance (either Euclidean, cosine or other) between them than words that have no semantic relationship. For example, words like “mom” and “dad” should be closer together than the words “mom” and “ketchup” or “dad” and “butter”.\n",
    "\n",
    "Word embeddings are created using a neural network with one input layer, one hidden layer and one output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0511d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Explain Continuous bag of words (CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c840cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in the middle. While in the Skip-gram model, the distributed representation of the input word is used to predict the context.\n",
    "\n",
    "A prerequisite for any neural network or any supervised training technique is to have labeled training data. How do you a train a neural network to predict word embedding when you don’t have any labeled data i.e words and their corresponding word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5838223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Explain SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d3147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word. Skip-gram is used to predict the context word for a given target word. It’s reverse of CBOW algorithm. Here, target word is input while context words are output. As there is more than one context word to be predicted which makesAs we can see w(t) is the target word or input given. There is one hidden layer which performs the dot product between the weight matrix and the input vector w(t). No activation function is used in the hidden layer. Now the result of the dot product at the hidden layer is passed to the output layer. Output layer computes the dot product between the output vector of the hidden layer and the weight matrix of the output layer. Then we apply the softmax activation function to compute the probability of words appearing to be in the context of w(t) at given context location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d9dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Explain Glove Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e320d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. The resulting embeddings show interesting linear substructures of the word in vector spac\n",
    "Text Preprocessing\n",
    "\n",
    "In this step, we will pre-process the text like removing the stop words, lemmatize the words etc. You can perform different steps based on your requirements. I will use nltk stopword corpus for stop word removal and nltk word lemmatization for finding lemmas. I order to use nltk corpus you will need to download it using the following commands. Downloading the corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

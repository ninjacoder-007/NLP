{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b834fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Explain the basic architecture of RNN cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf2b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "The fundamental feature of a Recurrent Neural Network (RNN) is that the network contains at least one feed-back connection, so the activations can flow round in a loop. That enables the networks to do temporal processing and learn sequences, e.g., perform sequence recognition/reproduction or temporal association/prediction. Recurrent neural network architectures can have many different forms. One common type consists of a standard Multi-Layer Perceptron (MLP) plus added loops. These can exploit the powerful non-linear mapping capabilities of the MLP, and also have some form of memory. Others have more uniform structures, potentially with every neuron connected to all the others, and may also have stochastic activation functions. For simple architectures and deterministic activation functions, learning can be achieved using similar gradient descent procedures to those leading to the back-propagation algorithm for feed-forward networks.\n",
    "\n",
    "In sequential tasks such as natural language and speech processing, there is always dependence of present input data upon the previous applied inputs. Task of RNNs is to find the relationship between current input and the previous applied inputs. In theory RNNs can make use of information sequence of any arbitrarily length, but in practice they are limited to looking back only a few steps.\n",
    "\n",
    "\n",
    "\n",
    "The above figure shows a RNN being unfolded into a full network. By unfolding we simply mean that we are repeating the same layer structure of network for the complete sequence. Xt is the input at time step t. Xt is a vector of any size N. A is the hidden state at time step t. It’s the “memory” of the network. It is calculated based on the previous hidden state and the input at the current step. Represented by At= f (W Xt +U At-1) Here W and U are weights for input and previous state value input. And f is the non-linearity applied to the sum to generate final cell state.\n",
    "\n",
    "One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they’d be extremely useful. Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in “the clouds are in the sky,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information. In theory, RNNs are absolutely capable of handling such “long-term dependencies.” A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don’t seem to be able to learn them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e88af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Explain Backpropagation through time (BPTT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f8ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Backpropagation Through Time, or BPTT, is the application of the Backpropagation training algorithm to recurrent neural network applied to sequence data like a time series.\n",
    "\n",
    "A recurrent neural network is shown one input each timestep and predicts one output.\n",
    "\n",
    "Conceptually, BPTT works by unrolling all input timesteps. Each timestep has one input timestep, one copy of the network, and one output. Errors are then calculated and accumulated for each timestep. The network is rolled back up and the weights are updated.\n",
    "\n",
    "Spatially, each timestep of the unrolled recurrent neural network may be seen as an additional layer given the order dependence of the problem and the internal state from the previous timestep is taken as an input on the subsequent timestep.\n",
    "\n",
    "We can summarize the algorithm as follows:\n",
    "\n",
    "Present a sequence of timesteps of input and output pairs to the network.\n",
    "Unroll the network then calculate and accumulate errors across each timestep.\n",
    "Roll-up the network and update weights. Repeat.\n",
    "BPTT can be computationally expensive as the number of timesteps increases.\n",
    "\n",
    "If input sequences are comprised of thousands of timesteps, then this will be the number of derivatives required for a single update weight update. This can cause weights to vanish or explode (go to zero or overflow) and make slow learning and model skill noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59475a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Explain Vanishing and exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ec68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Gradient is nothing but a derivative of loss function with respect to the weights. It is used to updates the weights to minimize the loss function during the back propagation in neural networks.\n",
    "\n",
    "What is Vanishing Gradients\n",
    "Vanishing Gradient occurs when the derivative or slope will get smaller and smaller as we go backward with every layer during backpropagation.\n",
    "\n",
    "When weights update is very small or exponential small, the training time takes too much longer, and in the worst case, this may completely stop the neural network training.\n",
    "\n",
    "A vanishing Gradient problem occurs with the sigmoid and tanh activation function because the derivatives of the sigmoid and tanh activation functions are between 0 to 0.25 and 0–1. Therefore, the updated weight values are small, and the new weight values are very similar to the old weight values. This leads to Vanishing Gradient problem. We can avoid this problem using the ReLU activation function because the gradient is 0 for negatives and zero input, and 1 for positive input.\n",
    "\n",
    "What is Exploding Gradients\n",
    "Exploding gradient occurs when the derivatives or slope will get larger and larger as we go backward with every layer during backpropagation. This situation is the exact opposite of the vanishing gradients.\n",
    "\n",
    "This problem happens because of weights, not because of the activation function. Due to high weight values, the derivatives will also higher so that the new weight varies a lot to the older weight, and the gradient will never converge. So it may result in oscillating around minima and never come to a global minima point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6881eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Explain Long short-term memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b371bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Backpropagation Through Time, or BPTT, is the application of the Backpropagation training algorithm to recurrent neural network applied to sequence data like a time series.\n",
    "\n",
    "A recurrent neural network is shown one input each timestep and predicts one output.\n",
    "\n",
    "Conceptually, BPTT works by unrolling all input timesteps. Each timestep has one input timestep, one copy of the network, and one output. Errors are then calculated and accumulated for each timestep. The network is rolled back up and the weights are updated.\n",
    "\n",
    "Spatially, each timestep of the unrolled recurrent neural network may be seen as an additional layer given the order dependence of the problem and the internal state from the previous timestep is taken as an input on the subsequent timestep.\n",
    "\n",
    "We can summarize the algorithm as follows:\n",
    "\n",
    "Present a sequence of timesteps of input and output pairs to the network.\n",
    "Unroll the network then calculate and accumulate errors across each timestep.\n",
    "Roll-up the network and update weights. Repeat.\n",
    "BPTT can be computationally expensive as the number of timesteps increases.\n",
    "If input sequences are comprised of thousands of timesteps, then this will be the number of derivatives required for a single update weight update. This can cause weights to vanish or explode (go to zero or overflow) and make slow learning and model skill noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd0e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Explain Gated recurrent unit (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582ec41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate. GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM. GRUs have been shown to exhibit better performance on certain smaller and less frequent datasets.\n",
    "\n",
    "Architecture There are several variations on the full gated unit, with gating done using the previous hidden state and the bias in various combinations, and a simplified form called minimal gated unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Explain Peephole LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Peephole Architectur\n",
    "Peephole connections refer to a modification to the basic LSTM architecture. ... Surprisingly, LSTM augmented by “peephole connections” from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes separated by either 50 or 49 discrete time steps.\n",
    "\n",
    "Until now we have seen simple LSTM network but this architecture is modified along with time in each and every research paper. One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96408e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04ae823",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bidirectional RNNs were introduced by [Schuster & Paliwal, 1997]. For a detailed discussion of the various architectures see also the paper [Graves & Schmidhuber, 2005]. Let us look at the specifics of such a network.\n",
    "\n",
    "For any time step t , given a minibatch input Xt∈Rn×d (number of examples: n , number of inputs in each example: d ) and let the hidden layer activation function be ϕ . In the bidirectional architecture, we assume that the forward and backward hidden states for this time step are H→t∈Rn×h and H←t∈Rn×h , respectively, where h is the number of hidden units. The forward and backward hidden state updates are as follows:\n",
    "\n",
    "H→tH←t=ϕ(XtW(f)xh+H→t−1W(f)hh+b(f)h),=ϕ(XtW(b)xh+H←t+1W(b)hh+b(b)h),\n",
    "\n",
    "where the weights W(f)xh∈Rd×h,W(f)hh∈Rh×h,W(b)xh∈Rd×h, and W(b)hh∈Rh×h , and biases b(f)h∈R1×h and b(b)h∈R1×h are all the model parameters.\n",
    "\n",
    "Next, we concatenate the forward and backward hidden states H→t and H←t to obtain the hidden state Ht∈Rn×2h to be fed into the output layer. In deep bidirectional RNNs with multiple hidden layers, such information is passed on as input to the next bidirectional layer. Last, the output layer computes the output Ot∈Rn×q (number of outputs: q ):\n",
    "\n",
    "Ot=HtWhq+bq.\n",
    "\n",
    "Here, the weight matrix Whq∈R2h×q and the bias bq∈R1×q are the model parameters of the output layer. In fact, the two directions can have different numbers of hidden units.\n",
    "\n",
    "If we want to have a mechanism in RNNs that offers comparable look-ahead ability as in hidden Markov models, we need to modify the RNN design that we have seen so far. Fortunately, this is easy conceptually. Instead of running an RNN only in the forward mode starting from the first token, we start another one from the last token running from back to front. Bidirectional RNNs add a hidden layer that passes information in a backward direction to more flexibly process such information. image.png In fact, this is not too dissimilar to the forward and backward recursions in the dynamic programing of hidden Markov models. The main distinction is that in the previous case these equations had a specific statistical meaning. Now they are devoid of such easily accessible interpretations and we can just treat them as generic and learnable functions. This transition epitomizes many of the principles guiding the design of modern deep networks: first, use the type of functional dependencies of classical statistical models, and then parameterize them in a generic form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf527f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Explain the gates of LSTM with equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "The basic difference between the architectures of RNNs and LSTMs is that the hidden layer of LSTM is a gated unit or gated cell. It consists of four layers that interact with one another in a way to produce the output of that cell along with the cell state. These two things are then passed onto the next hidden layer. Unlike RNNs which have got the only single neural net layer of tanh, LSTMs comprises of three logistic sigmoid gates and one tanh layer. Gates have been introduced in order to limit the information that is passed through the cell. They determine which part of the information will be needed by the next cell and which part is to be discarded. The output is usually in the range of 0-1 where ‘0’ means ‘reject all’ and ‘1’ means ‘include all’.\n",
    "\n",
    "image.png Each LSTM cell has three inputs h{t-1} ,C{t-1} and x_t and two outputs h_t and C_t . For a given time t, h_t is the hidden state, C_t is the cell state or memory, xt is the current data point or input. The first sigmoid layer has two inputs–h{t-1} and xt where h{t-1} is the hidden state of the previous cell. It is known as the forget gate as its output selects the amount of information of the previous cell to be included. The output is a number in [0,1] which is multiplied (point-wise) with the previous cell state C_{t-1} .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1101c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Explain BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0830924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bidirectional long-short term memory(bi-lstm) is the process of making any neural network o have the sequence information in both directions backwards (future to past) or forward(past to future).\n",
    "\n",
    "In bidirectional, our input flows in two directions, making a bi-lstm different from the regular LSTM. With the regular LSTM, we can make input flow in one direction, either backwards or forward. However, in bi-directional, we can make the input flow in both directions to preserve the future and the past information. For a better explanation, let’s have an example.\n",
    "\n",
    "In the sentence “boys go to …..” we can not fill the blank space. Still, when we have a future sentence “boys come out of school”, we can easily predict the past blank space the similar thing we want to perform by our model and bidirectional LSTM allows the neural network to perform this. image.png In the diagram, we can see the flow of information from backward and forward layers. BI-LSTM is usually employed where the sequence to sequence tasks are needed. This kind of network can be used in text classification, speech recognition and forecasting models. Next in the article, we are going to make a bi-directional LSTM model using python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a503c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Explain BiGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c55ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "A Bidirectional GRU, or BiGRU, is a sequence processing model that consists of two GRUs. one taking the input in a forward direction, and the other in a backwards direction. It is a bidirectional recurrent neural network with only the input and forget gates.\n",
    "\n",
    "Named entity recognition(NER) is one of the tasks of natural language processing(NLP). In view of the problem that the traditional character representation ability is weak and the neural network method is unable to capture the important sequence information. An self-attention-based bidirectional gated recurrent unit(BiGRU) and capsule network(CapsNet) for NER is proposed. This model generates character vectors through bidirectional encoder representation of transformers(BERT) pre-trained model. BiGRU is used to capture sequence context features, and self-attention mechanism is proposed to give different focus on the information captured by hidden layer of BiGRU. Finally, we propose to use CapsNet for entity recognition. We evaluated the recognition performance of the model on two datasets. Experimental results show that the model has better performance without relying on external dictionary information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

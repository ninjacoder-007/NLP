{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da42ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Explain the architecture of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b00a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT is basically an Encoder stack of transformer architecture. A transformer architecture is an encoder-decoder network that uses self-attention on the encoder side and attention on the decoder side. BERTBASE has 12 layers in the Encoder stack while BERTLARGE has 24 layers in the Encoder stack. These are more than the Transformer architecture described in the original paper (6 encoder layers). BERT architectures (BASE and LARGE) also have larger feedforward-networks (768 and 1024 hidden units respectively), and more attention heads (12 and 16 respectively) than the Transformer architecture suggested in the original paper. It contains 512 hidden units and 8 attention heads. BERTBASE contains 110M parameters while BERTLARGE has 340M parameters. \n",
    "BERTBASEand BERT LARGE architecture . This model takes CLS token as input first, then it is followed by a sequence of words as input. Here CLS is a classification token. It then passes the input to the above layers. Each layer applies self-attention, passes the result through a feedforward network after then it hands off to the next encoder.\n",
    "\n",
    "The model outputs a vector of hidden size (768 for BERT BASE). If we want to output a classifier from this model we can take the output corresponding to CLS token.\n",
    "\n",
    "image-2.png BERT output as Embeddings\n",
    "\n",
    "Now, this trained vector can be used to perform a number of tasks such as classification, translation, etc. For Example, the paper achieves great results just by using a single layer NN on the BERT model in the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afda20aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Explain Masked Language Modeling (MLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4666bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Masked Language Model (MLM) is the process how BERT was pre-trained. It has been shown, that to continue MLM on your own data can improve performances (see Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks). In our TSDAE-paper we also show that MLM is a powerful pre-training strategy for learning sentence embeddings. This is especially the case when you work on some specialized domain. image.png Note: Only running MLM will not yield good sentence embeddings. But you can first tune your favorite transformer model with MLM on your domain specific data. Then you can fine-tune the model with the labeled data you have or using other data sets like NLI, Paraphrases, or STS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70a3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Explain Next Sentence Prediction (NSP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence. image.png To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:\n",
    "\n",
    "A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\n",
    "\n",
    "A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.\n",
    "\n",
    "A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd05c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. What is Matthews evaluation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c425bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "Matthew's correlation coefficient, also abbreviated as MCC was invented by Brian Matthews in 1975. MCC is a statistical tool used for model evaluation. Its job is to gauge or measure the difference between the predicted values and actual values and is equivalent to chi-square statistics for a 2 x 2 contingency table\n",
    "\n",
    "\"A correlation of: C = 1 indicates perfect agreement, C = 0 is expected for a prediction no better than random, and C = -1 indicates total disagreement between prediction and observation\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What is Matthews Correlation Coefficient (MCC)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2052d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A significant percentage of the volume of protein crystals is occupied by solvent. ... Matthews defined VM, known as the Matthews coefficient, as the crystal volume per unit of protein molecular weight, and showed that VM bears a straightforward relationship to the fractional volume of solvent in the crystal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ebe2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Explain Semantic Role Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795f455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "In natural language processing, semantic role labeling (also called shallow semantic parsing or slot-filling) is the process that assigns labels to words or phrases in a sentence that indicates their semantic role in the sentence, such as that of an agent, goal, or result. It serves to find the meaning of the sentence. To do this, it detects the arguments associated with the predicate or verb of a sentence and how they are classified into their specific roles. A common example is the sentence \"Mary sold the book to John.\" The agent is \"Mary,\" the predicate is \"sold\" (or rather, \"to sell,\") the theme is \"the book,\" and the recipient is \"John.\" Another example is how \"the book belongs to me\" would need two labels such as \"possessed\" and \"possessor\" and \"the book was sold to John\" would need two other labels such as theme and recipient, despite these two clauses being similar to \"subject\" and \"object\" functions.[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06fac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Why Fine-tuning a BERT model takes less time than pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc49a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "We instead find that fine-tuning primarily affects the top layers of BERT, but with noteworthy variation across tasks. In particular, dependency parsing reconfigures most of the model, whereas SQuAD and MNLI appear to involve much shallower processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b1cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Recognizing Textual Entailment (RTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43817906",
   "metadata": {},
   "outputs": [],
   "source": [
    "Textual entailment recognition is the task of deciding, given two text fragments, whether the meaning of one text is entailed (can be inferred) from another text (see the Instructions tab for the specific operational definition of textual entailment assumed in the challenge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3cb360",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Explain the decoder stack of GPT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0838e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "After looking into transformers, BERT, and GPT-2, from what I understand, GPT-2 essentially uses only the decoder part of the original transformer architecture and uses masked self-attention that can only look at prior tokens.\n",
    "\n",
    "Why does GPT-2 not require the encoder part of the original transformer architecture?\n",
    "\n",
    "g GPT-2 is a close copy of the basic transformer architectureGPT-2 does not require the encoder part of the original transformer architecture as it is decoder-only, and there are no encoder attention blocks, so the decoder is equivalent to the encoder, except for the MASKING in the multi-head attention block, the decoder is only allowed to glean information from the prior words in the sentence. It works just like a traditional language model as it takes word vectors as input and produces estimates for the probability of the next word as outputs but it is auto-regressive as each token in the sentence has the context of the previous words. Thus GPT-2 works one token at a time.\n",
    "\n",
    "BERT, by contrast, is not auto-regressive. It uses the entire surrounding context all-at-once. GPT-2 the context vector is zero-initialized for the first word embedding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cba86f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What are Vanilla autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e55f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "The vanilla autoencoder, as proposed by Hinton, consists of only one hidden layer. The number of neurons in the hidden layer is less than the number of neurons in the input (or output) layer. This results in producing a bottleneck effect on the flow of information in the network, and therefore we can think of the hidden layer as a bottleneck layer, restricting the information that would be stored. Learning in the autoencoder consists of developing a compact representation of the input signal at the hidden layer so that the output layer can faithfully reproduce the original input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a380712",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What are Sparse autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2349f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "A Sparse Autoencoder is a type of autoencoder that employs sparsity to achieve an information bottleneck. Specifically the loss function is constructed so that activations are penalized within a layer. The sparsity constraint can be imposed with L1 regularization or a KL divergence between expected average neuron activation to an ideal distribution ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c2726",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. What are Denoising autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9069d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Denoising autoencoders are an extension of the basic autoencoder, and represent a stochastic version of it. Denoising autoencoders attempt to address identity-function risk by randomly corrupting input (i.e. introducing noise) that the autoencoder must then reconstruct, or denoise.\n",
    "\n",
    "Parameters and Corruption level\n",
    "\n",
    "The amount of noise to apply to the input takes the form of a percentage. Typically, 30 percent, or 0.3, is fine, but if you have very little data, you may want to consider adding more.\n",
    "\n",
    "Stacked Denoising Autoencoder\n",
    "\n",
    "A stacked denoising autoencoder is simply many denoising autoencoders strung together.\n",
    "\n",
    "It is to a denoising autoencoder what a deep-belief network is to a restricted Boltzmann machine.\n",
    "\n",
    "A key function of SDAs, and deep learning more generally, is unsupervised pre-training, layer by layer, as input is fed through. Once each layer is pre-trained to conduct feature selection and extraction on the input from the preceding layer, a second stage of supervised fine-tuning can follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628ba151",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. What are Convolutional autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcfe5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Convolutional Autoencoder is a variant of Convolutional Neural Networks that are used as the tools for unsupervised learning of convolution filters. They are generally applied in the task of image reconstruction to minimize reconstruction errors by learning the optimal filters. Once they are trained in this task, they can be applied to any input in order to extract features. Convolutional Autoencoders are general-purpose feature extractors differently from general autoencoders that completely ignore the 2D image structure. In autoencoders, the image must be unrolled into a single vector and the network must be built following the constraint on the number of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce2acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What are Stacked autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7069e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "A stacked autoencoder is a neural network consist several layers of sparse autoencoders where output of each hidden layer is connected to the input of the successive hidden layer.\n",
    "\n",
    "As shown in Figure above the hidden layers are trained by an unsupervised algorithm and then fine-tuned by a supervised method. Stacked autoencoder mainly consists of three steps[4].\n",
    "\n",
    "· Train autoencoder using input data and acquire the learned data. · The learned data from the previous layer is used as an input for the next layer and this continues until the training is completed. · Once all the hidden layers are trained use the backpropagation algorithm to minimize the cost function and weights are updated with the training set to achieve fine tuning.\n",
    "\n",
    "The recent advancements in Stacked Autoendocer is it provides a version of raw data with much detailed and promising feature information, which is used to train a classier with a specific context and find better accuracy than training with raw data. Stacked autoencoder improving accuracy in deep learning with noisy autoencoders embedded in the layers\n",
    "\n",
    "Stacked autoencoder are used for P300 Component Detection and Classification of 3D Spine Models in Adolescent Idiopathic Scoliosis in medical science. Classification of the rich and complex variability of spinal deformities is critical for comparisons between treatments and for long-term patient follow-ups.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "6. Explain how to generate sentences using LSTM autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0671de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "They use the model with video input data to both reconstruct sequences of frames of video as well as to predict frames of video, both of which are described as an unsupervised learning task.\n",
    "\n",
    "The input to the model is a sequence of vectors (image patches or features). The encoder LSTM reads in this sequence. After the last input has been read, the decoder LSTM takes over and outputs a prediction for the target sequence.\n",
    "\n",
    "More than simply using the model directly, the authors explore some interesting architecture choices that may help inform future applications of the model.\n",
    "\n",
    "They designed the model in such a way as to recreate the target sequence of video frames in reverse order, claiming that it makes the optimization problem solved by the model more tractable.\n",
    "\n",
    "The target sequence is same as the input sequence, but in reverse order. Reversing the target sequence makes the optimization easier because the model can get off the ground by looking at low range correlations.\n",
    "\n",
    "They also explore two approaches to training the decoder model, specifically a version conditioned in the previous output generated by the decoder, and another without any such conditioning.\n",
    "\n",
    "The decoder can be of two kinds – conditional or unconditioned. A conditional decoder receives the last generated output frame as input […]. An unconditioned decoder does not receive that input.\n",
    "\n",
    "A more elaborate autoencoder model was also explored where two decoder models were used for the one encoder: one to predict the next frame in the sequence and one to reconstruct frames in the sequence, referred to as a composite model.\n",
    "\n",
    "… reconstructing the input and predicting the future can be combined to create a composite […]. Here the encoder LSTM is asked to come up with a state from which we can both predict the next few frames as well as reconstruct the input.\n",
    "\n",
    "The models were evaluated in many ways, including using encoder to seed a classifier. It appears that rather than using the output of the encoder as an input for classification, they chose to seed a standalone LSTM classifier with the weights of the encoder model directly. This is surprising given the complication of the implementation.\n",
    "\n",
    "We initialize an LSTM classifier with the weights learned by the encoder LSTM from this model.\n",
    "\n",
    "The composite model without conditioning on the decoder was found to perform the best in their experiments.\n",
    "\n",
    "The best performing model was the Composite Model that combined an autoencoder and a future predictor. The conditional variants did not give any significant improvements in terms of classification accuracy after fine-tuning, however they did give slightly lower prediction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516715a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Explain Extractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad70537",
   "metadata": {},
   "outputs": [],
   "source": [
    "Extractive summarization aims at identifying the salient information that is then extracted and grouped together to form a concise summary. Abstractive summary generation rewrites the entire document by building internal semantic representation, and then a summary is created using natural language processing.\n",
    "\n",
    "Indicative versus informative—An indicative summary contains only the description of the spoken document and not the informative content. For example, the title page of books or reports. An informative summary contains the informative part of the original document. For example, research articles where the essential part of research is discussed.\n",
    "\n",
    "• Generic versus query-driven—In the query-driven approach, based on the given query, the information that is closely connected to the query is extracted. In the generic approach, the overall concept discussed in the document is presented.\n",
    "\n",
    "• Single versus multidocument—The summary can be generated from a single source document (or) from the multiple sources of a document.\n",
    "\n",
    "• Single versus multiple speakers—The summary is generated from the information presented by a single speaker or from multiple speakers where the speaker’s details are also incorporated in the summary.\n",
    "\n",
    "• Text-only versus multimodal—The summarization result can be presented either as text or as a speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4699ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Explain Abstractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Abstractive Summarization is a task in Natural Language Processing (NLP) that aims to generate a concise summary of a source text. Abstractive summarization yields a number of applications in different domains, from books and literature, to science and R&D, to financial research and legal documents analysis.\n",
    "\n",
    "From students to professionals, from digital publishers to content writers, the possibility to summarize pdf online could be very important to be more efficient in your work. Thanks to Artificial Intelligence algorithms, many tools are being born that helps you to automatic summarize a text. Those programmers use a variety of techniques to help machines understand natural language. Text summarization is a formidable challenge in the field of Natural Language Processing (NLP), indeed.\n",
    "\n",
    "The main techniques, as we have seen, are two: extraction and abstraction. A good automatic text summarization tool, nowadays, in most of the cases use only the extractive approach, while the abstractive one is the most interesting for the future. The abstractive method involves, as anticipated, the real ability for a machine to understand the semantics of the text and to create new phrases using natural language.\n",
    "\n",
    "PaperLit, tech company of Datrix Group, has created an automatic text summarization tool that applies part of the approaches described so far. The tool is used both in many solutions provided by PaperLit and as a free web tool for summarize pdf online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1879663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Explain Beam search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12699990",
   "metadata": {},
   "outputs": [],
   "source": [
    "A heuristic search algorithm that examines a graph by extending the most promising node in a limited set is known as beam search. Beam search is a heuristic search technique that always expands the W number of the best nodes at each level. It progresses level by level and moves downwards only from the best W nodes at each level. Beam Search uses breadth-first search to build its search tree. Beam Search constructs its search tree using breadth-first search. It generates all the successors of the current level’s state at each level of the tree. However, at each level, it only evaluates a W number of states. Other nodes are not taken into account.\n",
    "\n",
    "The heuristic cost associated with the node is used to choose the best nodes. The width of the beam search is denoted by W. If B is the branching factor, at every depth, there will always be W × B nodes under consideration, but only W will be chosen. More states are trimmed when the beam width is reduced. When W = 1, the search becomes a hill-climbing search in which the best node is always chosen from the successor nodes. No states are pruned if the beam width is unlimited, and the beam search is identified as a breadth-first search. The beamwidth bounds the amount of memory needed to complete the search, but it comes at the cost of completeness and optimality (possibly that it will not find the best solution). The reason for this danger is that the desired state could have been pruned.\n",
    "\n",
    "Example: The search tree generated using this algorithm with W = 2 & B = 3 is given below :\n",
    "        One simple length normalization formula is to divide the number of occurrences by the length of the document. For example, we can measure the length in pages and divide the number of occurrences (term frequency) by the number of pages as seen in Column 4 above. Dividing the number of occurrences by the number of pages increases the score of the 32 page document relative to the dictionary and index because it has many more occurrences per page of the word “water.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b95c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. Explain Coverage normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddec58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the field of linguistics and NLP, Morpheme is defined as a base form of the word. A token is basically made up of two components one is morphemes and the other is inflectional formlike prefix or suffix.\n",
    "\n",
    "For example, consider the word Antinationalist (Anti + national+ ist ) which is made up of Anti and ist as inflectional forms and national as the morpheme.\n",
    "\n",
    "Normalization is the process of converting a token into its base form. In the normalization process, the inflectional form of a word is removed so that the base form can be obtained. So in our above example, the normal form of antinationalist is national.\n",
    "\n",
    "Normalization is helpful in reducing the number of unique tokens present in the text, removing the variations in a text. and also cleaning the text by removing redundant information.\n",
    "\n",
    "Two popular methods used for normalization are stemming and lemmatization. Let’s discuss them in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05409b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "12. Explain ROUGE metric evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab1dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It is essentially a set of metrics for evaluating automatic summarization of texts as well as machine translations.\n",
    "\n",
    "It works by comparing an automatically produced summary or translation against a set of reference summaries (typically human-produced). Let’s say that we have the following system and reference summaries:\n",
    "\n",
    "System Summary (what the machine produced):\n",
    "\n",
    "the cat was found under the bed\n",
    "\n",
    "Reference Summary (gold standard — usually by humans):\n",
    "\n",
    "the cat was under the bed\n",
    "\n",
    "If we consider just the individual words, the number of overlapping words between the system summary and reference summary is 6. This, however, does not tell you much as a metric. To get a good quantitative value, we can actually compute the precision and recall using the overlap.\n",
    "\n",
    "Simply put, recall (in the context of ROUGE) refers to how much of the reference summary the system summary is recovering or capturing. If we are just considering the individual words, it can be computed "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
